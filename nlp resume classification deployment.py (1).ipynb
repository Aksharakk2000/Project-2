{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1493fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile app.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "from io import BytesIO\n",
    "import pickle\n",
    "import click\n",
    "import spacy\n",
    "import docx2txt\n",
    "import pdfplumber\n",
    "from pickle import load\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import sklearn\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import pickle as pk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Specify NLTK data path\n",
    "# nltk_data_path = os.path.join(os.environ['APPDATA'], 'nltk_data')\n",
    "\n",
    "# Set NLTK data path\n",
    "# nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "\n",
    "# Now import and download the necessary NLTK resources\n",
    "# nltk.download('punkt', download_dir=nltk_data_path)\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')\n",
    "# # nltk.download('stopwords', download_dir=nltk_data_path)\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "en_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# en_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# FUNCTIONS\n",
    "def getText(filename):\n",
    "    # Create empty string \n",
    "    fullText = ''\n",
    "    if filename.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        fullText = docx2txt.process(filename)\n",
    "    else:  \n",
    "        with pdfplumber.open(filename) as pdf_file:\n",
    "            page = pdf_file.pages[0]\n",
    "            fullText = page.extract_text()\n",
    "    return fullText\n",
    "\n",
    "\n",
    "def display(doc_file):\n",
    "    resume = []\n",
    "    if doc_file.type == \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\":\n",
    "        resume.append(docx2txt.process(doc_file))\n",
    "    else:\n",
    "        with pdfplumber.open(doc_file) as pdf:\n",
    "            pages = pdf.pages[0]\n",
    "            resume.append(pages.extract_text())\n",
    "    return resume\n",
    "\n",
    "def preprocess(sentence):\n",
    "    sentence = str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    sentence = sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    rem_url = re.sub(r'http\\S+', '', cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', rem_url)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 2 if not w in en_stopwords]\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemma_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "    return \" \".join(lemma_words) \n",
    "\n",
    "\n",
    "\n",
    "file_type = pd.DataFrame([], columns=['Uploaded File',  'Predicted Profile'])\n",
    "\n",
    "\n",
    "filename = []\n",
    "predicted = []\n",
    "\n",
    "#MAX_FILE_SIZE_MB = 2  # Maximum file size allowed in MB\n",
    "\n",
    "\n",
    "# Load the trained model and vectorizer\n",
    "model = pickle.load(open('model.pkl','rb'))\n",
    "tfidf = pickle.load(open('vectorizer.pkl','rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Streamlit app\n",
    "\n",
    "st.title('NLP Model Deployment')\n",
    "    \n",
    "MAX_FILE_SIZE_MB = 2  # Maximum file size allowed in MB\n",
    "\n",
    "upload_file = st.file_uploader('Upload Resume',type=['txt','pdf','docx'],accept_multiple_files=True)\n",
    "\n",
    "\n",
    "\n",
    "for doc_file in upload_file:\n",
    "    if doc_file is not None:\n",
    "        # Check file size\n",
    "        if len(doc_file.getvalue()) > MAX_FILE_SIZE_MB * 1024 * 1024:\n",
    "            st.error(f\"File '{doc_file.name}' exceeds the maximum size limit of {MAX_FILE_SIZE_MB} MB.\")\n",
    "            continue\n",
    "        \n",
    "        filename.append(doc_file.name)\n",
    "        cleaned = preprocess(display(doc_file))\n",
    "        prediction = model.predict(tfidf.transform([cleaned]))[0]\n",
    "        predicted.append(prediction)\n",
    "        # extText = len(doc_file.pages)\n",
    "\n",
    "        # extText = getText(doc_file)\n",
    "        # len(reader.pages) \n",
    "        \n",
    "if len(predicted) > 0:\n",
    "    # Define a mapping dictionary to map label encoded values to original categories\n",
    "     label_mapping = {\n",
    "        0: 'PeopleSoft',\n",
    "        1: 'React JS Developer',\n",
    "        2: 'SQL Developer',\n",
    "        3: 'Workday'\n",
    "        # Add more mappings as needed\n",
    "     }\n",
    "\n",
    "    \n",
    "    \n",
    "  \n",
    "    # Predict and map the labels to their original categories\n",
    "predicted_categories = [label_mapping[label] for label in predicted]\n",
    "\n",
    "    # Update the DataFrame with the original category predictions\n",
    "file_type['Uploaded File'] = filename\n",
    "file_type['Predicted Profile'] = predicted_categories\n",
    "\n",
    "    # Display the updated DataFrame\n",
    "st.table(file_type.style.format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6387a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!streamlit run app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007aad02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
